
\chapter{Methodology}
One of the mains problem in the field of optical flow is the performance of models  on unseen domains, which often requires additional training data and training in order to perform well \cite{hurOpticalFlowEstimation2020}. In this scenario, this should not be a problem, since the domain is clearly defined and no unseen domain should be encountered.

\section{Data acquisition}
Data acquisition poses a significant problem for this project.  Changes in german privacy law have made it, understandably, more difficult for researchers to gain access to medical data. 
Patients have to agree to provide the data gathered by a PET scan to specific research projects \cite{specht-riemenschneiderImAuftragBundesministeriums}.
%TODO: Check citation
Furthermore, even if real world data were widely available, there is still a problem when it comes to deep learning models. Most architectures that predict optical flow rely on ground truth data
during the training process. At the time of writing, the top performing models on the spring dataset consist of almost solely these supervised models \cite{mehlSpringHighResolutionHighDetail2023}.
With PET scans, no ground truth data is available, since it is not possible to accurately associate each pair of voxels on between two corresponding frames. A possible solution is the use of generated data. For generated data, the exact ground truth is always known and can be used for the training process. Naturally, this can limit the performance of the model on real data, since the generated data will probably not be a an exact model of real data. For this thesis, virtual patient models were used. The models were generated by the XCAT software, which can generate data for medical imaging research. The models include variable organ volumes and body sizes, as well as models for heartbeat and respiratory motions. Additionally, it is possible to add anomalies which correspond to medical problems \cite{4DExtendedCardiacTorso}.
\section{Preprocessing}
In order to make the generated xcat data more comparable to real data, varying noise levels were added to the data.
For each generated body, noise levels of 0, 10,  20, 50 and 100 were applied, resulting in data of varying quality.
%TODO SPECIFY noise levels

\subsection{Background removal}
\label{ss:bgremoval}
 If the model would be trained on perfect data, noise that will be present in real PET scans could strongly impact performance.
Since a high level of background noise may still \enquote{irritate} the model, two approaches for removal of background noise were considered.
Each voxel $v \in \mathbb{R}$ has a value corresponding to its brightness and is either part of the model body $Bo$ or of the background $Bg$.
 The first approach used a combination of thresholding and chain approximation in order to classify each voxel. First the OTSU thresholding method \cite{otsuThresholdSelectionMethod1979} was used in order to make the body better discernible from the background. Afterwards, the image is dilated by a $7\times 7$ kernel in order to increase the size of the \enquote{blob}. This prevents parts of the body that would have otherwise been seen as part of the background from being removed. Then, multiple contours are detected using chain approximation and all but the largest contour are discarded. Based on the largest contour a bounding box $BB$ is then created and again dilated, which covers the body. Now all voxels are set to:
 $$v=\begin{cases}
 	v \text{ if } v \in BB\\
 	0 \text{ if } v \notin BB
 \end{cases}$$
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/background/before.png}
		\caption{2d slice of frame}
		\label{fig:bgbefore}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/background/after.png}
		\caption{biggest contour}
		\label{fig:bgcontour}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/background/convex_hull 1.png}
		\caption{Bounding rectangle}
		\label{fig:bgrect}
	\end{subfigure}
	\caption{First approach for background removal}
	\label{fig:bgremoval1}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/background/3d.png}
	\caption{Top down view on the prediction results on 3d data. White corresponds to the background, blue corresponds to the body. The darker the color, the more voxels in the column are assigned the class \enquote{body}}
	\label{fig:background3d}
\end{figure}

After first evaluation on 3d data, a second approach was taken. The second approach is a supervised model that assigns each value to either $Bo$ or  $Bg$. The exact design can be found in. %TODO: ADD DESIGN TO APPENDIX.
The model seems to work well on generated data, as seen in figure \eqref{fig:background3d}, and should work similarly on real data, since it does not need to detect fine details that may not be given on real data.
\subsection{Centred data}
Since all acquired data has the body centred, it is easier for the model to overfit, since all data points are very similar. In order to reduce this effect, the volumes were semi-randomly cropped to a smaller size. Only  areas which do not contain information about the body were cropped. This has the advantage of reducing computational power necessary, as well as moving the body to different positions. The height remained unchanged, since the body takes up the entire height. In the other directions, a lot of empty space can be removed without consequences.
\subsection{Data Normalization}
Since real world and generated data have vastly different value ranges, the data was scaled to $[0,1]$
\section{Model architectures}
Multiple architectures were considered. In order to increase the speed of training, approaches were initially applied on two-dimensional slices of xcat data.
\subsection{FlowFormer}
In the repository for FlowFormer\cite{drinkingcoderFlowFormerTransformerArchitecture2023} code for training the network on new data is provided. This allows for an easy training setup. Since this approach was not continued after initial evaluation, no significant adaptions to the architecture were made.
\subsection{Based on FlowNetS}
Since the prediction of optical flow in three-dimensional space is much more resource intensive when compared to  two-dimensional space, a small model architecture was necessary.

Initially, the structure of the simple approach of FlowNet 2.0 \cite{ilgFlowNetEvolutionOptical2016} was adapted in order to work with the two-dimensional slices. This was achieved using a padding for the images to make the input data divisible by 64.
Multiple approaches were tried in order to improve the quality of the results.
\subsubsection{Batch Normalization}
To test the effect of Batch Normalization on overfitting, many approaches were tested with Batch Normalization enabled and disabled.
\subsubsection{Dropout layer}
A variety of positions and amounts for dropout layers were tested. After extensive testing, a single dropout layer was placed after the final fully connected layer.
\subsubsection{Brightness scaling}
Initially, a strong flicker was present in the given data. In order to reduce this flickering, the total brightness of the first images in a series of images was taken and all succeeding images were scaled to the same total brightness. This approach was dropped after an error source was found in the data.
\subsubsection{Loss function}
\label{sss:lossfunctions}
For FlowNet 2.0 the \acs{EPE} was used. For this thesis, this loss function has been replaced with a linear combination of multiple loss functions.
The first loss function is the \ac{MSE} which is calculated as
$$MSE=\dfrac{1}{n}\sum_{i=1}^{n}\left(Y_i -\hat{Y}_i\right)^2.$$
Additionally, perceptual loss \eqref{ss:perceptualloss} was tested as a part of the loss function.
As a third part of the loss function, brightness constancy loss was used, which for the sake of completeness can be written as \begin{equation}
L_\text{perceptual} = \Vert \phi(y_\text{label}) - \phi (y_\text{out})\Vert^2_2.
\end{equation}


 The  brightness constancy loss function calculates the differences between the target and the warped source images and combines them using a sum. Initially, the sum of squared differences was used, which was replaced with the root mean sqaure error.
The loss function can be written as
\begin{equation}
	\text{brightness constancy loss} = \sqrt{(\sum_{i} ( \text{target}_i - \text{warped}_i )^2)}
\end{equation}
with target$_i$ being the target frame and warped$_i$ being the base image with the predicted flow vectors applied.

The three loss functions were combined using the formula \begin{equation}
	\text{loss}=\alpha \times  \text{MSE} + \beta \times  \text{perceptual loss} + \gamma \times  \text{brightness constancy loss}
\end{equation}
$\\\text{  with }\alpha, \beta, \gamma \in \mathbb{R} \text{ such that }  \alpha + \beta + \gamma = 1.$
On three-dimensional data, the perceptual loss was omitted for reasons that will be explained in section \eqref{} %TODO where explained?.
\subsection{FlowNet-PET}
In order to combine the unsupervised approach of FlowNet-PET with a supervised approach, the two architectures needed to be compatible. Since both approaches are based on FlowNetS\cite{Flownet2pytorch2023}, only small changes were necessary. 
%TODO Describe changes when done.

Initially, the supervised architecture proposed in this thesis was used in order to pretrain a checkpoint for optical flow estimation. Afterwards, as proposed by Cetinic et al. \cite{cetinicFinetuningConvolutionalNeural2018}, all layers except the first convolutional layer were retrained using the unsupervised approach proposed by O'Briain et al. \cite{obriainFlowNetPETUnsupervisedLearning2022}.